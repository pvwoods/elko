{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2383808f-0bf0-45a3-b06f-59474b2223b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from typing import Union, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06f395fd-eaeb-479b-8bc1-416ca51e4edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 64])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SSResidualUnit(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels:int, dilation:int):\n",
    "        \n",
    "        super(SSResidualUnit, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.dilation = dilation\n",
    "        \n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, kernel_size=7, dilation=dilation, padding=\"same\"),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(channels, channels, kernel_size=1, padding=\"same\")\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.sequence(x)\n",
    "        return x + h\n",
    "    \n",
    "s = SSResidualUnit(2, 1)\n",
    "x = torch.randn((1, 2, 64))\n",
    "s(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "423f3b7c-3186-473f-967a-22e392331241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SSEncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels:int, stride:int):\n",
    "        \n",
    "        super(SSEncoderBlock, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.resid_0 = SSResidualUnit(self.channels // 2, dilation = 1)\n",
    "        self.resid_1 = SSResidualUnit(self.channels // 2, dilation = 3)\n",
    "        self.resid_2 = SSResidualUnit(self.channels // 2, dilation = 9)\n",
    "        \n",
    "        self.out_conv = nn.Conv1d(self.channels // 2, self.channels, kernel_size=2*self.stride, stride=self.stride, padding=self.stride//2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.resid_0(x)\n",
    "        x1 = self.resid_0(x0)\n",
    "        x2 = self.resid_0(x1)\n",
    "        out = self.out_conv(x2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "se = SSEncoderBlock(4, 2)\n",
    "se(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "65c2faf5-9d4c-4ede-b422-994caee1a348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 51])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SSEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int, channels:int, embedding_dims:int):\n",
    "        \n",
    "        super(SSEncoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.channels = channels\n",
    "        self.embedding_dims = embedding_dims\n",
    "        \n",
    "        self.in_conv = nn.Conv1d(self.in_channels, self.channels, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.encoder_block_0 = SSEncoderBlock(self.channels * 2, 2)\n",
    "        self.encoder_block_1 = SSEncoderBlock(self.channels * 4, 4)\n",
    "        self.encoder_block_2 = SSEncoderBlock(self.channels * 8, 5)\n",
    "        self.encoder_block_3 = SSEncoderBlock(self.channels * 16, 8)\n",
    "        \n",
    "        self.out_conv = nn.Conv1d(self.channels * 16, self.embedding_dims, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = self.in_conv(x)\n",
    "        h0 = self.encoder_block_0(h)\n",
    "        h1 = self.encoder_block_1(h0)\n",
    "        h2 = self.encoder_block_2(h1)\n",
    "        h3 = self.encoder_block_3(h2)\n",
    "        \n",
    "        out = self.out_conv(h3)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "sse = SSEncoder(1, 4, 128)\n",
    "sse(torch.randn(1, 1, 128*128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "83b21a49-7ae3-44bd-ae7d-334598f381b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SSDecoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels:int, stride:int):\n",
    "        \n",
    "        super(SSDecoderBlock, self).__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.in_conv = nn.ConvTranspose1d(self.channels, self.channels // 2, kernel_size=2*self.stride, stride=self.stride, padding=self.stride//2)\n",
    "        \n",
    "        self.resid_0 = SSResidualUnit(self.channels // 2, dilation = 1)\n",
    "        self.resid_1 = SSResidualUnit(self.channels // 2, dilation = 3)\n",
    "        self.resid_2 = SSResidualUnit(self.channels // 2, dilation = 9)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = self.in_conv(x)\n",
    "        x1 = self.resid_0(x0)\n",
    "        x2 = self.resid_1(x1)\n",
    "        out = self.resid_2(x2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "se = SSDecoderBlock(8, 2)\n",
    "x = torch.randn((1, 8, 64))\n",
    "se(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b92e2-5e80-41ef-a684-a0c7e6356b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSDecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int, channels:int, embedding_dims:int):\n",
    "        \n",
    "        super(SSDecoder, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.channels = channels\n",
    "        self.embedding_dims = embedding_dims\n",
    "        \n",
    "        self.in_conv = nn.Conv1d(self.in_channels, self.channels, kernel_size=7, padding=3)\n",
    "        \n",
    "        self.encoder_block_0 = SSEncoderBlock(self.channels * 2, 2)\n",
    "        self.encoder_block_1 = SSEncoderBlock(self.channels * 4, 4)\n",
    "        self.encoder_block_2 = SSEncoderBlock(self.channels * 8, 5)\n",
    "        self.encoder_block_3 = SSEncoderBlock(self.channels * 16, 8)\n",
    "        \n",
    "        self.out_conv = nn.Conv1d(self.channels * 16, self.embedding_dims, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = self.in_conv(x)\n",
    "        h0 = self.encoder_block_0(h)\n",
    "        h1 = self.encoder_block_1(h0)\n",
    "        h2 = self.encoder_block_2(h1)\n",
    "        h3 = self.encoder_block_3(h2)\n",
    "        \n",
    "        out = self.out_conv(h3)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d74a819e-f92d-4d29-81b5-58261e84a85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 51])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is not the quantizer they used, just a simple one for experimenting\n",
    "class EmbeddingQuantizer(nn.Module):\n",
    "\n",
    "    def __init__(self, codebook_size:int, embedding_dims:int) -> None:\n",
    "        \n",
    "        super(EmbeddingQuantizer, self).__init__()\n",
    "\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.codebook_size = codebook_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(self.codebook_size, self.embedding_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        B, C, T = x.shape\n",
    "\n",
    "        reshape_inputs = x.permute(0, 2, 1).contiguous() # embed by channel values (BTC)\n",
    "        reshape_inputs = reshape_inputs.view(-1, self.embedding_dims) # reshape to embedding dimensions (B, E)\n",
    "\n",
    "        # calculate distances between all inputs and embeddings\n",
    "        xs = (reshape_inputs**2).sum(dim=1, keepdim=True)\n",
    "        ys = (self.embeddings.weight**2).sum(dim=1)\n",
    "        dots = reshape_inputs @ self.embeddings.weight.t()\n",
    "        distances = (xs + ys) - (2 * dots)\n",
    "\n",
    "        # get embedding indices and quantize\n",
    "        embedding_indexes = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        quantized_embeddings = self.embeddings(embedding_indexes).view(B, T, C).permute(0, 2, 1).contiguous()\n",
    "\n",
    "        loss = F.mse_loss(x, quantized_embeddings)\n",
    "\n",
    "        return quantized_embeddings, embedding_indexes.squeeze(-1), loss\n",
    "\n",
    "eq = EmbeddingQuantizer(1024, 128)\n",
    "x = torch.randn((1, 128, 51))\n",
    "embs, codes, loss = eq(x)\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd895a10-d480-430c-91c7-683a5cef3fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 51]),\n",
       " tensor([254, 254, 388, 388, 254, 388, 254, 388, 254, 388, 388, 388, 254, 388,\n",
       "         254, 254, 388, 254, 388, 388, 254, 254, 388, 254, 388, 388, 388, 254,\n",
       "         254, 388, 254, 388, 254, 254, 388, 132, 254, 254, 254, 254, 254, 254,\n",
       "         254, 254, 388, 254, 254, 254, 388, 254, 254]),\n",
       " tensor(0.6970, grad_fn=<MseLossBackward0>))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SoundStream(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int, channels:int, embedding_dims:int, codebook_size:int):\n",
    "        \n",
    "        super(SoundStream, self).__init__()\n",
    "        \n",
    "        self.encoder = SSEncoder(in_channels, channels, embedding_dims)\n",
    "        self.quantizer = EmbeddingQuantizer(codebook_size, embedding_dims)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h = self.encoder(x)\n",
    "        embs, codes, q_loss = self.quantizer(h)\n",
    "        \n",
    "        return embs, codes, q_loss\n",
    "    \n",
    "model = SoundStream(1, 4, 128, 512)\n",
    "embs, codes, q_loss = model(torch.randn(1, 1, 128*128))\n",
    "embs.shape, codes, q_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
