{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ff4fbb05-79f9-4bad-ac0c-cf6dbb507817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolm_pytorch.data import exists, cast_tuple, collate_one_or_multiple_tensors, curtail_to_multiple, curtail_to_shortest_collate, pad_to_longest_fn\n",
    "from functools import partial, wraps\n",
    "\n",
    "from beartype.typing import Tuple\n",
    "from beartype.door import is_bearable\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.functional import resample\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from audiolm_pytorch.utils import curtail_to_multiple\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        exts = ['mp3', 'wav'],\n",
    "        max_length = None,\n",
    "        target_sample_hz = None,\n",
    "        seq_len_multiple_of = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        path = Path(folder)\n",
    "        assert path.exists(), 'folder does not exist'\n",
    "\n",
    "        files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(path)\n",
    "             for name in files\n",
    "             if name.endswith(tuple(exts))]\n",
    "        assert len(files) > 0, 'no sound files found'\n",
    "\n",
    "        self.files = files\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.target_sample_hz = cast_tuple(target_sample_hz)\n",
    "        self.seq_len_multiple_of = seq_len_multiple_of\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        \n",
    "        try:\n",
    "            data, sample_hz = torchaudio.load(file) # FMA dataset is a mess, so just jumping over bad loads for now\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "        if exists(self.target_sample_hz) and self.target_sample_hz[0] != sample_hz:\n",
    "            data = resample(data, orig_freq=sample_hz, new_freq=self.target_sample_hz[0])\n",
    "        \n",
    "        if data.shape[0] != 1:\n",
    "            data = data.mean(dim=0, keepdims=True)\n",
    "        \n",
    "        if data.size(1) > self.max_length:\n",
    "            max_start = data.size(1) - self.max_length\n",
    "            start = torch.randint(0, max_start, (1, ))\n",
    "            data = data[:, start:start + self.max_length]\n",
    "\n",
    "        else:\n",
    "            data = torch.nn.functional.pad(data, (0, self.max_length - data.size(1)), 'constant')\n",
    "        \n",
    "        data = rearrange(data, '1 ... -> ...')\n",
    "\n",
    "        num_outputs = len(self.target_sample_hz)\n",
    "        data = cast_tuple(data, num_outputs)\n",
    "\n",
    "        if exists(self.max_length):\n",
    "            data = tuple(d[:self.max_length] for d in data)\n",
    "\n",
    "        if exists(self.seq_len_multiple_of):\n",
    "            data = tuple(curtail_to_multiple(d, self.seq_len_multiple_of) for d in data)\n",
    "\n",
    "        data = tuple(d.float() for d in data)\n",
    "\n",
    "        if num_outputs == 1:\n",
    "            return data[0]\n",
    "\n",
    "        return\n",
    "    \n",
    "def get_dataloader(ds, pad_to_longest = True, **kwargs):\n",
    "    dual_collate_fn = pad_to_longest_fn if pad_to_longest else curtail_to_shortest_collate\n",
    "    def collate_fn(batch):\n",
    "        batch = list(filter(lambda x: x is not None, batch))\n",
    "        return dual_collate_fn(batch)\n",
    "    \n",
    "    return DataLoader(ds, collate_fn = collate_fn, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64eba6c3-6423-41be-806a-e3626ec0445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import copy\n",
    "from random import choice\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "from beartype.typing import Union, List, Optional, Tuple\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from beartype import beartype\n",
    "from beartype.door import is_bearable\n",
    "from beartype.vale import Is\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from audiolm_pytorch.optimizer import get_optimizer\n",
    "\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from audiolm_pytorch.soundstream import SoundStream\n",
    "\n",
    "from audiolm_pytorch.audiolm_pytorch import (\n",
    "    SemanticTransformer,\n",
    "    SemanticTransformerWrapper,\n",
    "    CoarseTransformer,\n",
    "    CoarseTransformerWrapper,\n",
    "    FineTransformer,\n",
    "    FineTransformerWrapper,\n",
    "    FairseqVQWav2Vec,\n",
    "    HubertWithKmeans\n",
    ")\n",
    "\n",
    "from audiolm_pytorch.data import SoundDataset\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from audiolm_pytorch.trainer import (\n",
    "    noop,\n",
    "    cycle,\n",
    "    yes_or_no,\n",
    "    accum_log,\n",
    "    has_duplicates,\n",
    "    determine_types,\n",
    "    DEFAULT_SAMPLE_RATE\n",
    ")\n",
    "\n",
    "class CustomSoundStreamTrainer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        soundstream: SoundStream,\n",
    "        *,\n",
    "        num_train_steps,\n",
    "        batch_size,\n",
    "        data_max_length = None,\n",
    "        folder='',\n",
    "        lr = 3e-4,\n",
    "        grad_accum_every = 4,\n",
    "        wd = 0.,\n",
    "        max_grad_norm = 0.5,\n",
    "        discr_max_grad_norm = None,\n",
    "        save_results_every = 100,\n",
    "        save_model_every = 1000,\n",
    "        results_folder = './results',\n",
    "        valid_frac = 0.05,\n",
    "        random_split_seed = 42,\n",
    "        ema_beta = 0.995,\n",
    "        ema_update_after_step = 500,\n",
    "        ema_update_every = 10,\n",
    "        apply_grad_penalty_every = 4,\n",
    "        accelerate_kwargs: dict = dict(),\n",
    "        dataset:Dataset = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.accelerator = Accelerator(**accelerate_kwargs)\n",
    "\n",
    "        self.soundstream = soundstream\n",
    "        self.ema_soundstream = EMA(soundstream, beta = ema_beta, update_after_step = ema_update_after_step, update_every = ema_update_every)\n",
    "\n",
    "        self.register_buffer('steps', torch.Tensor([0]))\n",
    "\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.grad_accum_every = grad_accum_every\n",
    "\n",
    "        # optimizers\n",
    "\n",
    "        self.optim = get_optimizer(soundstream.non_discr_parameters(), lr = lr, wd = wd)\n",
    "\n",
    "        for discr_optimizer_key, discr in self.multiscale_discriminator_iter():\n",
    "            one_multiscale_discr_optimizer = get_optimizer(discr.parameters(), lr = lr, wd = wd)\n",
    "            setattr(self, discr_optimizer_key, one_multiscale_discr_optimizer)\n",
    "\n",
    "        self.discr_optim = get_optimizer(soundstream.stft_discriminator.parameters(), lr = lr, wd = wd)\n",
    "\n",
    "        # max grad norm\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.discr_max_grad_norm = discr_max_grad_norm\n",
    "\n",
    "        # create dataset\n",
    "        \n",
    "        if dataset is not None:\n",
    "            self.ds = dataset\n",
    "        else:\n",
    "            self.ds = SoundDataset(\n",
    "                folder,\n",
    "                max_length = data_max_length,\n",
    "                target_sample_hz = soundstream.target_sample_hz,\n",
    "                seq_len_multiple_of = soundstream.seq_len_multiple_of\n",
    "            )\n",
    "\n",
    "        # split for validation\n",
    "\n",
    "        if valid_frac > 0:\n",
    "            train_size = int((1 - valid_frac) * len(self.ds))\n",
    "            valid_size = len(self.ds) - train_size\n",
    "            self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n",
    "            self.print(f'training with dataset of {len(self.ds)} samples and validating with randomly splitted {len(self.valid_ds)} samples')\n",
    "        else:\n",
    "            self.valid_ds = self.ds\n",
    "            self.print(f'training with shared training and valid dataset of {len(self.ds)} samples')\n",
    "\n",
    "        # dataloader\n",
    "\n",
    "        self.dl = get_dataloader(self.ds, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        self.valid_dl = get_dataloader(self.valid_ds, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        # prepare with accelerator\n",
    "\n",
    "        (\n",
    "            self.soundstream,\n",
    "            self.optim,\n",
    "            self.discr_optim,\n",
    "            self.dl,\n",
    "            self.valid_dl\n",
    "        ) = self.accelerator.prepare(\n",
    "            self.soundstream,\n",
    "            self.optim,\n",
    "            self.discr_optim,\n",
    "            self.dl,\n",
    "            self.valid_dl\n",
    "        )\n",
    "\n",
    "        # prepare the multiscale discriminators with accelerator\n",
    "\n",
    "        for name, _ in self.multiscale_discriminator_iter():\n",
    "            optimizer = getattr(self, name)\n",
    "            optimizer = self.accelerator.prepare(optimizer)\n",
    "            setattr(self, name, optimizer)\n",
    "\n",
    "        # dataloader iterators\n",
    "\n",
    "        self.dl_iter = cycle(self.dl)\n",
    "        self.valid_dl_iter = cycle(self.valid_dl)\n",
    "\n",
    "        self.save_model_every = save_model_every\n",
    "        self.save_results_every = save_results_every\n",
    "\n",
    "        self.apply_grad_penalty_every = apply_grad_penalty_every\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "\n",
    "        if len([*self.results_folder.glob('**/*')]) > 0 and yes_or_no('do you want to clear previous experiment checkpoints and results?'):\n",
    "            rmtree(str(self.results_folder))\n",
    "\n",
    "        self.results_folder.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    def save(self, path):\n",
    "        pkg = dict(\n",
    "            model = self.accelerator.get_state_dict(self.soundstream),\n",
    "            ema_model = self.ema_soundstream.state_dict(),\n",
    "            optim = self.optim.state_dict(),\n",
    "            discr_optim = self.discr_optim.state_dict()\n",
    "        )\n",
    "\n",
    "        for key, _ in self.multiscale_discriminator_iter():\n",
    "            discr_optim = getattr(self, key)\n",
    "            pkg[key] = discr_optim.state_dict()\n",
    "\n",
    "        torch.save(pkg, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        path = Path(path)\n",
    "        assert path.exists()\n",
    "        pkg = torch.load(str(path))\n",
    "\n",
    "        soundstream = self.accelerator.unwrap_model(self.soundstream)\n",
    "        soundstream.load_state_dict(pkg['model'])\n",
    "\n",
    "        self.ema_soundstream.load_state_dict(pkg['ema_model'])\n",
    "        self.optim.load_state_dict(pkg['optim'])\n",
    "        self.discr_optim.load_state_dict(pkg['discr_optim'])\n",
    "\n",
    "        for key, _ in self.multiscale_discriminator_iter():\n",
    "            discr_optim = getattr(self, key)\n",
    "            discr_optim.load_state_dict(pkg[key])\n",
    "\n",
    "    def multiscale_discriminator_iter(self):\n",
    "        for ind, discr in enumerate(self.soundstream.discriminators):\n",
    "            yield f'multiscale_discr_optimizer_{ind}', discr\n",
    "\n",
    "    def print(self, msg):\n",
    "        self.accelerator.print(msg)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    @property\n",
    "    def is_distributed(self):\n",
    "        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n",
    "\n",
    "    @property\n",
    "    def is_main(self):\n",
    "        return self.accelerator.is_main_process\n",
    "\n",
    "    @property\n",
    "    def is_local_main(self):\n",
    "        return self.accelerator.is_local_main_process\n",
    "\n",
    "    def train_step(self):\n",
    "        device = self.device\n",
    "\n",
    "        steps = int(self.steps.item())\n",
    "        apply_grad_penalty = not (steps % self.apply_grad_penalty_every)\n",
    "\n",
    "        self.soundstream.train()\n",
    "\n",
    "        # logs\n",
    "\n",
    "        logs = {}\n",
    "\n",
    "        # update vae (generator)\n",
    "\n",
    "        for _ in range(self.grad_accum_every):\n",
    "            wave, = next(self.dl_iter)\n",
    "            wave = wave.to(device)\n",
    "\n",
    "            loss, (recon_loss, *_) = self.soundstream(wave, return_loss_breakdown = True)\n",
    "\n",
    "            self.accelerator.backward(loss / self.grad_accum_every)\n",
    "\n",
    "            accum_log(logs, dict(\n",
    "                loss = loss.item() / self.grad_accum_every,\n",
    "                recon_loss = recon_loss / self.grad_accum_every\n",
    "            ))\n",
    "\n",
    "        if exists(self.max_grad_norm):\n",
    "            self.accelerator.clip_grad_norm_(self.soundstream.parameters(), self.max_grad_norm)\n",
    "\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        # update discriminator\n",
    "\n",
    "        for _ in range(self.grad_accum_every):\n",
    "            wave, = next(self.dl_iter)\n",
    "            wave = wave.to(device)\n",
    "\n",
    "            discr_losses = self.soundstream(\n",
    "                wave,\n",
    "                apply_grad_penalty = apply_grad_penalty,\n",
    "                return_discr_loss = True,\n",
    "                return_discr_losses_separately = True\n",
    "            )\n",
    "\n",
    "            for name, discr_loss in discr_losses:\n",
    "                self.accelerator.backward(discr_loss / self.grad_accum_every, retain_graph = True)\n",
    "                accum_log(logs, {name: discr_loss.item() / self.grad_accum_every})\n",
    "\n",
    "        if exists(self.discr_max_grad_norm):\n",
    "            self.accelerator.clip_grad_norm_(self.soundstream.stft_discriminator.parameters(), self.discr_max_grad_norm)\n",
    "\n",
    "        # gradient step for all discriminators\n",
    "\n",
    "        self.discr_optim.step()\n",
    "        self.discr_optim.zero_grad()\n",
    "\n",
    "        for ind in range(len(self.soundstream.discriminators)):\n",
    "            discr_optimizer = getattr(self, f'multiscale_discr_optimizer_{ind}')\n",
    "            discr_optimizer.step()\n",
    "            discr_optimizer.zero_grad()\n",
    "\n",
    "        # build pretty printed losses\n",
    "\n",
    "        losses_str = f\"{steps}: soundstream total loss: {logs['loss']:.3f}, soundstream recon loss: {logs['recon_loss']:.3f}\"\n",
    "\n",
    "        for key, loss in logs.items():\n",
    "            if not key.startswith('scale:'):\n",
    "                continue\n",
    "            _, scale_factor = key.split(':')\n",
    "\n",
    "            losses_str += f\" | discr (scale {scale_factor}) loss: {loss:.3f}\"\n",
    "\n",
    "        # log\n",
    "\n",
    "        self.print(losses_str)\n",
    "\n",
    "        # update exponential moving averaged generator\n",
    "\n",
    "        if self.is_main:\n",
    "            self.ema_soundstream.update()\n",
    "\n",
    "        # sample results every so often\n",
    "\n",
    "        if self.is_main and not (steps % self.save_results_every):\n",
    "            for model, filename in ((self.ema_soundstream.ema_model, f'{steps}.ema'), (self.soundstream, str(steps))):\n",
    "                model.eval()\n",
    "\n",
    "                wave, = next(self.valid_dl_iter)\n",
    "                wave = wave.to(device)\n",
    "\n",
    "                recons = model(wave, return_recons_only = True)\n",
    "\n",
    "                milestone = steps // self.save_results_every\n",
    "\n",
    "                for ind, recon in enumerate(recons.unbind(dim = 0)):\n",
    "                    filename = str(self.results_folder / f'sample_{steps}.flac')\n",
    "                    torchaudio.save(filename, recon.cpu().detach(), DEFAULT_SAMPLE_RATE)\n",
    "\n",
    "            self.print(f'{steps}: saving to {str(self.results_folder)}')\n",
    "\n",
    "        # save model every so often\n",
    "\n",
    "        if self.is_main and not (steps % self.save_model_every):\n",
    "            state_dict = self.soundstream.state_dict()\n",
    "            model_path = str(self.results_folder / f'soundstream.{steps}.pt')\n",
    "            torch.save(state_dict, model_path)\n",
    "\n",
    "            ema_state_dict = self.ema_soundstream.state_dict()\n",
    "            model_path = str(self.results_folder / f'soundstream.{steps}.ema.pt')\n",
    "            torch.save(ema_state_dict, model_path)\n",
    "\n",
    "            self.print(f'{steps}: saving model to {str(self.results_folder)}')\n",
    "\n",
    "        self.steps += 1\n",
    "        return logs\n",
    "\n",
    "    def train(self, log_fn = noop):\n",
    "\n",
    "        while self.steps < self.num_train_steps:\n",
    "            logs = self.train_step()\n",
    "            log_fn(logs)\n",
    "\n",
    "        self.print('training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "da749fab-fbd4-4e54-87d9-f9f0acb1cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 7597 samples and validating with randomly splitted 400 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: soundstream total loss: 28.402, soundstream recon loss: 0.066 | discr (scale 1) loss: 2.000 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 1.999\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: soundstream total loss: 31.701, soundstream recon loss: 0.064 | discr (scale 1) loss: 1.997 | discr (scale 0.5) loss: 1.996 | discr (scale 0.25) loss: 1.994\n",
      "2: soundstream total loss: 25.800, soundstream recon loss: 0.045 | discr (scale 1) loss: 2.008 | discr (scale 0.5) loss: 2.010 | discr (scale 0.25) loss: 2.016\n",
      "3: soundstream total loss: 28.514, soundstream recon loss: 0.054 | discr (scale 1) loss: 2.005 | discr (scale 0.5) loss: 2.006 | discr (scale 0.25) loss: 2.023\n",
      "4: soundstream total loss: 31.809, soundstream recon loss: 0.054 | discr (scale 1) loss: 1.996 | discr (scale 0.5) loss: 1.996 | discr (scale 0.25) loss: 2.021\n",
      "5: soundstream total loss: 27.483, soundstream recon loss: 0.040 | discr (scale 1) loss: 1.956 | discr (scale 0.5) loss: 1.962 | discr (scale 0.25) loss: 1.966\n",
      "6: soundstream total loss: 33.554, soundstream recon loss: 0.059 | discr (scale 1) loss: 1.929 | discr (scale 0.5) loss: 1.940 | discr (scale 0.25) loss: 1.930\n",
      "7: soundstream total loss: 26.043, soundstream recon loss: 0.039 | discr (scale 1) loss: 1.878 | discr (scale 0.5) loss: 1.896 | discr (scale 0.25) loss: 1.843\n",
      "8: soundstream total loss: 31.162, soundstream recon loss: 0.065 | discr (scale 1) loss: 1.860 | discr (scale 0.5) loss: 1.888 | discr (scale 0.25) loss: 1.785\n",
      "9: soundstream total loss: 24.697, soundstream recon loss: 0.040 | discr (scale 1) loss: 1.811 | discr (scale 0.5) loss: 1.848 | discr (scale 0.25) loss: 1.659\n",
      "10: soundstream total loss: 23.188, soundstream recon loss: 0.042 | discr (scale 1) loss: 1.623 | discr (scale 0.5) loss: 1.706 | discr (scale 0.25) loss: 1.383\n",
      "11: soundstream total loss: 21.250, soundstream recon loss: 0.041 | discr (scale 1) loss: 1.396 | discr (scale 0.5) loss: 1.491 | discr (scale 0.25) loss: 1.126\n",
      "12: soundstream total loss: 28.777, soundstream recon loss: 0.072 | discr (scale 1) loss: 1.264 | discr (scale 0.5) loss: 1.390 | discr (scale 0.25) loss: 0.931\n",
      "13: soundstream total loss: 28.849, soundstream recon loss: 0.073 | discr (scale 1) loss: 1.213 | discr (scale 0.5) loss: 1.322 | discr (scale 0.25) loss: 1.010\n",
      "14: soundstream total loss: 29.052, soundstream recon loss: 0.058 | discr (scale 1) loss: 0.607 | discr (scale 0.5) loss: 0.740 | discr (scale 0.25) loss: 0.544\n",
      "15: soundstream total loss: 25.497, soundstream recon loss: 0.048 | discr (scale 1) loss: 1.361 | discr (scale 0.5) loss: 1.194 | discr (scale 0.25) loss: 0.729\n",
      "16: soundstream total loss: 21.995, soundstream recon loss: 0.037 | discr (scale 1) loss: 0.536 | discr (scale 0.5) loss: 0.625 | discr (scale 0.25) loss: 0.597\n",
      "17: soundstream total loss: 18.155, soundstream recon loss: 0.026 | discr (scale 1) loss: 1.913 | discr (scale 0.5) loss: 0.941 | discr (scale 0.25) loss: 1.119\n",
      "18: soundstream total loss: 24.409, soundstream recon loss: 0.043 | discr (scale 1) loss: 1.159 | discr (scale 0.5) loss: 0.767 | discr (scale 0.25) loss: 0.656\n",
      "19: soundstream total loss: 28.405, soundstream recon loss: 0.048 | discr (scale 1) loss: 0.901 | discr (scale 0.5) loss: 0.386 | discr (scale 0.25) loss: 2.208\n",
      "20: soundstream total loss: 25.288, soundstream recon loss: 0.045 | discr (scale 1) loss: 1.284 | discr (scale 0.5) loss: 0.402 | discr (scale 0.25) loss: 1.045\n",
      "21: soundstream total loss: 32.142, soundstream recon loss: 0.060 | discr (scale 1) loss: 0.708 | discr (scale 0.5) loss: 0.634 | discr (scale 0.25) loss: 0.561\n",
      "22: soundstream total loss: 22.211, soundstream recon loss: 0.038 | discr (scale 1) loss: 0.850 | discr (scale 0.5) loss: 0.786 | discr (scale 0.25) loss: 1.022\n",
      "23: soundstream total loss: 21.991, soundstream recon loss: 0.035 | discr (scale 1) loss: 0.423 | discr (scale 0.5) loss: 1.233 | discr (scale 0.25) loss: 0.871\n",
      "24: soundstream total loss: 34.159, soundstream recon loss: 0.058 | discr (scale 1) loss: 1.378 | discr (scale 0.5) loss: 1.307 | discr (scale 0.25) loss: 1.443\n",
      "25: soundstream total loss: 29.494, soundstream recon loss: 0.046 | discr (scale 1) loss: 1.972 | discr (scale 0.5) loss: 1.495 | discr (scale 0.25) loss: 1.064\n",
      "26: soundstream total loss: 27.088, soundstream recon loss: 0.052 | discr (scale 1) loss: 1.853 | discr (scale 0.5) loss: 2.027 | discr (scale 0.25) loss: 2.780\n",
      "27: soundstream total loss: 28.415, soundstream recon loss: 0.058 | discr (scale 1) loss: 1.939 | discr (scale 0.5) loss: 2.261 | discr (scale 0.25) loss: 4.040\n",
      "28: soundstream total loss: 29.101, soundstream recon loss: 0.078 | discr (scale 1) loss: 2.944 | discr (scale 0.5) loss: 1.951 | discr (scale 0.25) loss: 2.541\n",
      "29: soundstream total loss: 25.058, soundstream recon loss: 0.065 | discr (scale 1) loss: 3.688 | discr (scale 0.5) loss: 3.193 | discr (scale 0.25) loss: 2.977\n",
      "30: soundstream total loss: 30.310, soundstream recon loss: 0.073 | discr (scale 1) loss: 2.649 | discr (scale 0.5) loss: 2.144 | discr (scale 0.25) loss: 2.220\n",
      "31: soundstream total loss: 33.710, soundstream recon loss: 0.099 | discr (scale 1) loss: 0.896 | discr (scale 0.5) loss: 1.461 | discr (scale 0.25) loss: 2.441\n",
      "32: soundstream total loss: 25.265, soundstream recon loss: 0.064 | discr (scale 1) loss: 1.368 | discr (scale 0.5) loss: 3.217 | discr (scale 0.25) loss: 2.844\n",
      "33: soundstream total loss: 29.030, soundstream recon loss: 0.075 | discr (scale 1) loss: 0.896 | discr (scale 0.5) loss: 2.559 | discr (scale 0.25) loss: 1.882\n",
      "34: soundstream total loss: 28.200, soundstream recon loss: 0.085 | discr (scale 1) loss: 0.639 | discr (scale 0.5) loss: 0.523 | discr (scale 0.25) loss: 2.019\n",
      "35: soundstream total loss: 30.776, soundstream recon loss: 0.102 | discr (scale 1) loss: 0.107 | discr (scale 0.5) loss: 0.368 | discr (scale 0.25) loss: 0.967\n",
      "36: soundstream total loss: 28.589, soundstream recon loss: 0.080 | discr (scale 1) loss: 0.086 | discr (scale 0.5) loss: 0.088 | discr (scale 0.25) loss: 2.901\n",
      "37: soundstream total loss: 31.350, soundstream recon loss: 0.099 | discr (scale 1) loss: 0.482 | discr (scale 0.5) loss: 0.588 | discr (scale 0.25) loss: 8.276\n",
      "38: soundstream total loss: 28.961, soundstream recon loss: 0.083 | discr (scale 1) loss: 1.118 | discr (scale 0.5) loss: 3.053 | discr (scale 0.25) loss: 20.088\n",
      "39: soundstream total loss: 30.036, soundstream recon loss: 0.085 | discr (scale 1) loss: 4.122 | discr (scale 0.5) loss: 7.510 | discr (scale 0.25) loss: 39.428\n",
      "40: soundstream total loss: 28.433, soundstream recon loss: 0.076 | discr (scale 1) loss: 10.533 | discr (scale 0.5) loss: 12.588 | discr (scale 0.25) loss: 54.009\n",
      "41: soundstream total loss: 29.732, soundstream recon loss: 0.119 | discr (scale 1) loss: 11.477 | discr (scale 0.5) loss: 10.781 | discr (scale 0.25) loss: 41.364\n",
      "42: soundstream total loss: 35.970, soundstream recon loss: 0.124 | discr (scale 1) loss: 5.014 | discr (scale 0.5) loss: 5.084 | discr (scale 0.25) loss: 14.552\n",
      "43: soundstream total loss: 35.021, soundstream recon loss: 0.085 | discr (scale 1) loss: 4.723 | discr (scale 0.5) loss: 4.567 | discr (scale 0.25) loss: 1.876\n",
      "44: soundstream total loss: 29.728, soundstream recon loss: 0.049 | discr (scale 1) loss: 5.993 | discr (scale 0.5) loss: 4.823 | discr (scale 0.25) loss: 2.549\n",
      "45: soundstream total loss: 26.381, soundstream recon loss: 0.043 | discr (scale 1) loss: 3.672 | discr (scale 0.5) loss: 2.741 | discr (scale 0.25) loss: 1.521\n",
      "46: soundstream total loss: 22.225, soundstream recon loss: 0.036 | discr (scale 1) loss: 3.471 | discr (scale 0.5) loss: 3.008 | discr (scale 0.25) loss: 3.689\n",
      "47: soundstream total loss: 23.625, soundstream recon loss: 0.048 | discr (scale 1) loss: 3.128 | discr (scale 0.5) loss: 3.695 | discr (scale 0.25) loss: 5.572\n",
      "48: soundstream total loss: 22.309, soundstream recon loss: 0.046 | discr (scale 1) loss: 5.526 | discr (scale 0.5) loss: 5.511 | discr (scale 0.25) loss: 8.489\n",
      "49: soundstream total loss: 26.865, soundstream recon loss: 0.066 | discr (scale 1) loss: 7.488 | discr (scale 0.5) loss: 6.497 | discr (scale 0.25) loss: 9.740\n",
      "50: soundstream total loss: 20.604, soundstream recon loss: 0.048 | discr (scale 1) loss: 8.209 | discr (scale 0.5) loss: 6.407 | discr (scale 0.25) loss: 9.591\n",
      "51: soundstream total loss: 25.760, soundstream recon loss: 0.069 | discr (scale 1) loss: 7.667 | discr (scale 0.5) loss: 5.250 | discr (scale 0.25) loss: 7.869\n",
      "52: soundstream total loss: 22.614, soundstream recon loss: 0.060 | discr (scale 1) loss: 6.064 | discr (scale 0.5) loss: 3.731 | discr (scale 0.25) loss: 6.084\n",
      "53: soundstream total loss: 29.085, soundstream recon loss: 0.076 | discr (scale 1) loss: 4.213 | discr (scale 0.5) loss: 3.018 | discr (scale 0.25) loss: 5.514\n",
      "54: soundstream total loss: 24.543, soundstream recon loss: 0.060 | discr (scale 1) loss: 3.518 | discr (scale 0.5) loss: 2.879 | discr (scale 0.25) loss: 4.467\n",
      "55: soundstream total loss: 27.021, soundstream recon loss: 0.072 | discr (scale 1) loss: 3.811 | discr (scale 0.5) loss: 2.900 | discr (scale 0.25) loss: 4.466\n",
      "56: soundstream total loss: 32.045, soundstream recon loss: 0.078 | discr (scale 1) loss: 3.402 | discr (scale 0.5) loss: 2.704 | discr (scale 0.25) loss: 3.835\n",
      "57: soundstream total loss: 32.347, soundstream recon loss: 0.079 | discr (scale 1) loss: 3.064 | discr (scale 0.5) loss: 2.525 | discr (scale 0.25) loss: 3.407\n",
      "58: soundstream total loss: 28.635, soundstream recon loss: 0.069 | discr (scale 1) loss: 2.541 | discr (scale 0.5) loss: 2.413 | discr (scale 0.25) loss: 2.914\n",
      "59: soundstream total loss: 28.317, soundstream recon loss: 0.064 | discr (scale 1) loss: 2.259 | discr (scale 0.5) loss: 2.285 | discr (scale 0.25) loss: 2.573\n",
      "60: soundstream total loss: 30.362, soundstream recon loss: 0.058 | discr (scale 1) loss: 2.293 | discr (scale 0.5) loss: 2.325 | discr (scale 0.25) loss: 2.585\n",
      "61: soundstream total loss: 28.928, soundstream recon loss: 0.078 | discr (scale 1) loss: 2.279 | discr (scale 0.5) loss: 2.438 | discr (scale 0.25) loss: 2.747\n",
      "62: soundstream total loss: 25.879, soundstream recon loss: 0.055 | discr (scale 1) loss: 2.071 | discr (scale 0.5) loss: 2.139 | discr (scale 0.25) loss: 2.126\n",
      "63: soundstream total loss: 30.159, soundstream recon loss: 0.072 | discr (scale 1) loss: 2.045 | discr (scale 0.5) loss: 2.248 | discr (scale 0.25) loss: 2.428\n",
      "64: soundstream total loss: 25.920, soundstream recon loss: 0.043 | discr (scale 1) loss: 1.945 | discr (scale 0.5) loss: 1.996 | discr (scale 0.25) loss: 2.036\n",
      "65: soundstream total loss: 23.354, soundstream recon loss: 0.052 | discr (scale 1) loss: 1.912 | discr (scale 0.5) loss: 1.985 | discr (scale 0.25) loss: 2.037\n",
      "66: soundstream total loss: 21.562, soundstream recon loss: 0.035 | discr (scale 1) loss: 1.858 | discr (scale 0.5) loss: 1.893 | discr (scale 0.25) loss: 1.857\n",
      "67: soundstream total loss: 24.267, soundstream recon loss: 0.053 | discr (scale 1) loss: 1.780 | discr (scale 0.5) loss: 1.802 | discr (scale 0.25) loss: 1.693\n",
      "68: soundstream total loss: 20.587, soundstream recon loss: 0.037 | discr (scale 1) loss: 1.818 | discr (scale 0.5) loss: 1.820 | discr (scale 0.25) loss: 1.726\n",
      "69: soundstream total loss: 25.238, soundstream recon loss: 0.048 | discr (scale 1) loss: 1.790 | discr (scale 0.5) loss: 1.813 | discr (scale 0.25) loss: 1.545\n",
      "70: soundstream total loss: 25.015, soundstream recon loss: 0.038 | discr (scale 1) loss: 1.844 | discr (scale 0.5) loss: 1.859 | discr (scale 0.25) loss: 1.535\n",
      "71: soundstream total loss: 24.839, soundstream recon loss: 0.048 | discr (scale 1) loss: 1.771 | discr (scale 0.5) loss: 1.778 | discr (scale 0.25) loss: 1.297\n",
      "72: soundstream total loss: 22.346, soundstream recon loss: 0.037 | discr (scale 1) loss: 1.843 | discr (scale 0.5) loss: 1.964 | discr (scale 0.25) loss: 1.586\n",
      "73: soundstream total loss: 28.826, soundstream recon loss: 0.064 | discr (scale 1) loss: 1.829 | discr (scale 0.5) loss: 1.821 | discr (scale 0.25) loss: 1.550\n",
      "74: soundstream total loss: 22.087, soundstream recon loss: 0.033 | discr (scale 1) loss: 1.826 | discr (scale 0.5) loss: 1.845 | discr (scale 0.25) loss: 1.496\n",
      "75: soundstream total loss: 21.991, soundstream recon loss: 0.042 | discr (scale 1) loss: 1.774 | discr (scale 0.5) loss: 1.726 | discr (scale 0.25) loss: 1.282\n",
      "76: soundstream total loss: 24.096, soundstream recon loss: 0.043 | discr (scale 1) loss: 1.795 | discr (scale 0.5) loss: 1.739 | discr (scale 0.25) loss: 1.326\n",
      "77: soundstream total loss: 29.796, soundstream recon loss: 0.059 | discr (scale 1) loss: 1.802 | discr (scale 0.5) loss: 1.733 | discr (scale 0.25) loss: 1.350\n",
      "78: soundstream total loss: 22.666, soundstream recon loss: 0.042 | discr (scale 1) loss: 1.869 | discr (scale 0.5) loss: 1.790 | discr (scale 0.25) loss: 1.506\n",
      "79: soundstream total loss: 21.084, soundstream recon loss: 0.039 | discr (scale 1) loss: 1.844 | discr (scale 0.5) loss: 1.729 | discr (scale 0.25) loss: 1.401\n",
      "80: soundstream total loss: 21.783, soundstream recon loss: 0.039 | discr (scale 1) loss: 1.785 | discr (scale 0.5) loss: 1.731 | discr (scale 0.25) loss: 1.354\n",
      "81: soundstream total loss: 29.077, soundstream recon loss: 0.056 | discr (scale 1) loss: 1.827 | discr (scale 0.5) loss: 1.745 | discr (scale 0.25) loss: 1.446\n",
      "82: soundstream total loss: 23.530, soundstream recon loss: 0.039 | discr (scale 1) loss: 1.809 | discr (scale 0.5) loss: 1.629 | discr (scale 0.25) loss: 1.386\n",
      "83: soundstream total loss: 26.127, soundstream recon loss: 0.068 | discr (scale 1) loss: 1.745 | discr (scale 0.5) loss: 1.601 | discr (scale 0.25) loss: 1.372\n",
      "84: soundstream total loss: 24.108, soundstream recon loss: 0.045 | discr (scale 1) loss: 1.756 | discr (scale 0.5) loss: 1.617 | discr (scale 0.25) loss: 1.165\n",
      "85: soundstream total loss: 21.664, soundstream recon loss: 0.035 | discr (scale 1) loss: 1.845 | discr (scale 0.5) loss: 1.706 | discr (scale 0.25) loss: 1.423\n",
      "86: soundstream total loss: 25.062, soundstream recon loss: 0.052 | discr (scale 1) loss: 1.810 | discr (scale 0.5) loss: 1.606 | discr (scale 0.25) loss: 1.249\n",
      "87: soundstream total loss: 23.304, soundstream recon loss: 0.046 | discr (scale 1) loss: 1.721 | discr (scale 0.5) loss: 1.478 | discr (scale 0.25) loss: 1.051\n",
      "88: soundstream total loss: 27.089, soundstream recon loss: 0.051 | discr (scale 1) loss: 1.653 | discr (scale 0.5) loss: 1.395 | discr (scale 0.25) loss: 0.970\n",
      "89: soundstream total loss: 23.876, soundstream recon loss: 0.037 | discr (scale 1) loss: 1.697 | discr (scale 0.5) loss: 1.459 | discr (scale 0.25) loss: 0.802\n",
      "90: soundstream total loss: 28.341, soundstream recon loss: 0.056 | discr (scale 1) loss: 1.689 | discr (scale 0.5) loss: 1.459 | discr (scale 0.25) loss: 0.850\n",
      "91: soundstream total loss: 26.105, soundstream recon loss: 0.041 | discr (scale 1) loss: 1.664 | discr (scale 0.5) loss: 1.374 | discr (scale 0.25) loss: 0.824\n",
      "92: soundstream total loss: 27.633, soundstream recon loss: 0.053 | discr (scale 1) loss: 1.624 | discr (scale 0.5) loss: 1.317 | discr (scale 0.25) loss: 0.781\n",
      "93: soundstream total loss: 18.800, soundstream recon loss: 0.033 | discr (scale 1) loss: 1.663 | discr (scale 0.5) loss: 1.445 | discr (scale 0.25) loss: 0.976\n",
      "94: soundstream total loss: 29.560, soundstream recon loss: 0.076 | discr (scale 1) loss: 1.556 | discr (scale 0.5) loss: 1.394 | discr (scale 0.25) loss: 0.590\n",
      "95: soundstream total loss: 29.189, soundstream recon loss: 0.070 | discr (scale 1) loss: 1.424 | discr (scale 0.5) loss: 1.312 | discr (scale 0.25) loss: 0.445\n",
      "96: soundstream total loss: 21.647, soundstream recon loss: 0.032 | discr (scale 1) loss: 1.571 | discr (scale 0.5) loss: 1.910 | discr (scale 0.25) loss: 0.566\n",
      "97: soundstream total loss: 26.578, soundstream recon loss: 0.045 | discr (scale 1) loss: 1.430 | discr (scale 0.5) loss: 2.116 | discr (scale 0.25) loss: 0.568\n",
      "98: soundstream total loss: 28.361, soundstream recon loss: 0.050 | discr (scale 1) loss: 1.389 | discr (scale 0.5) loss: 3.094 | discr (scale 0.25) loss: 0.747\n",
      "99: soundstream total loss: 26.177, soundstream recon loss: 0.053 | discr (scale 1) loss: 1.343 | discr (scale 0.5) loss: 3.672 | discr (scale 0.25) loss: 0.874\n",
      "100: soundstream total loss: 28.913, soundstream recon loss: 0.047 | discr (scale 1) loss: 1.275 | discr (scale 0.5) loss: 2.656 | discr (scale 0.25) loss: 0.922\n",
      "100: saving to results\n",
      "101: soundstream total loss: 22.337, soundstream recon loss: 0.041 | discr (scale 1) loss: 1.366 | discr (scale 0.5) loss: 2.099 | discr (scale 0.25) loss: 0.885\n",
      "102: soundstream total loss: 20.058, soundstream recon loss: 0.033 | discr (scale 1) loss: 1.200 | discr (scale 0.5) loss: 1.618 | discr (scale 0.25) loss: 0.667\n",
      "103: soundstream total loss: 22.374, soundstream recon loss: 0.046 | discr (scale 1) loss: 1.035 | discr (scale 0.5) loss: 1.412 | discr (scale 0.25) loss: 0.578\n",
      "104: soundstream total loss: 27.765, soundstream recon loss: 0.054 | discr (scale 1) loss: 0.866 | discr (scale 0.5) loss: 1.260 | discr (scale 0.25) loss: 0.462\n",
      "105: soundstream total loss: 27.504, soundstream recon loss: 0.061 | discr (scale 1) loss: 0.973 | discr (scale 0.5) loss: 1.509 | discr (scale 0.25) loss: 0.669\n",
      "106: soundstream total loss: 27.784, soundstream recon loss: 0.067 | discr (scale 1) loss: 0.914 | discr (scale 0.5) loss: 1.510 | discr (scale 0.25) loss: 0.919\n",
      "107: soundstream total loss: 23.227, soundstream recon loss: 0.039 | discr (scale 1) loss: 1.166 | discr (scale 0.5) loss: 1.650 | discr (scale 0.25) loss: 1.722\n",
      "108: soundstream total loss: 24.125, soundstream recon loss: 0.047 | discr (scale 1) loss: 0.924 | discr (scale 0.5) loss: 1.633 | discr (scale 0.25) loss: 1.293\n",
      "109: soundstream total loss: 26.788, soundstream recon loss: 0.049 | discr (scale 1) loss: 0.854 | discr (scale 0.5) loss: 1.668 | discr (scale 0.25) loss: 1.592\n",
      "110: soundstream total loss: 17.833, soundstream recon loss: 0.031 | discr (scale 1) loss: 0.741 | discr (scale 0.5) loss: 1.527 | discr (scale 0.25) loss: 1.834\n",
      "111: soundstream total loss: 22.937, soundstream recon loss: 0.045 | discr (scale 1) loss: 0.661 | discr (scale 0.5) loss: 1.825 | discr (scale 0.25) loss: 2.188\n",
      "112: soundstream total loss: 23.491, soundstream recon loss: 0.048 | discr (scale 1) loss: 0.893 | discr (scale 0.5) loss: 2.509 | discr (scale 0.25) loss: 2.744\n",
      "113: soundstream total loss: 25.739, soundstream recon loss: 0.049 | discr (scale 1) loss: 0.931 | discr (scale 0.5) loss: 3.711 | discr (scale 0.25) loss: 4.622\n",
      "114: soundstream total loss: 32.798, soundstream recon loss: 0.070 | discr (scale 1) loss: 0.685 | discr (scale 0.5) loss: 3.918 | discr (scale 0.25) loss: 4.980\n",
      "115: soundstream total loss: 22.913, soundstream recon loss: 0.039 | discr (scale 1) loss: 0.346 | discr (scale 0.5) loss: 2.466 | discr (scale 0.25) loss: 2.310\n",
      "116: soundstream total loss: 30.474, soundstream recon loss: 0.048 | discr (scale 1) loss: 0.264 | discr (scale 0.5) loss: 1.939 | discr (scale 0.25) loss: 2.214\n",
      "117: soundstream total loss: 28.277, soundstream recon loss: 0.059 | discr (scale 1) loss: 0.247 | discr (scale 0.5) loss: 1.557 | discr (scale 0.25) loss: 1.995\n",
      "118: soundstream total loss: 22.918, soundstream recon loss: 0.048 | discr (scale 1) loss: 0.066 | discr (scale 0.5) loss: 1.601 | discr (scale 0.25) loss: 2.168\n",
      "119: soundstream total loss: 25.866, soundstream recon loss: 0.061 | discr (scale 1) loss: 0.001 | discr (scale 0.5) loss: 1.403 | discr (scale 0.25) loss: 1.574\n",
      "120: soundstream total loss: 24.083, soundstream recon loss: 0.049 | discr (scale 1) loss: 0.669 | discr (scale 0.5) loss: 1.778 | discr (scale 0.25) loss: 1.531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 26\u001b[0m\n\u001b[1;32m     10\u001b[0m dataset \u001b[38;5;241m=\u001b[39m MusicDataset(\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/philip/ferous/FMA/fma_small/\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     12\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mDATA_MAX_LENGTH,\n\u001b[1;32m     13\u001b[0m     target_sample_hz \u001b[38;5;241m=\u001b[39m soundstream\u001b[38;5;241m.\u001b[39mtarget_sample_hz,\n\u001b[1;32m     14\u001b[0m     seq_len_multiple_of \u001b[38;5;241m=\u001b[39m soundstream\u001b[38;5;241m.\u001b[39mseq_len_multiple_of\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomSoundStreamTrainer(\n\u001b[1;32m     18\u001b[0m     soundstream,\n\u001b[1;32m     19\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset\n\u001b[1;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 348\u001b[0m, in \u001b[0;36mCustomSoundStreamTrainer.train\u001b[0;34m(self, log_fn)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_fn \u001b[38;5;241m=\u001b[39m noop):\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_steps:\n\u001b[0;32m--> 348\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m         log_fn(logs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining complete\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 250\u001b[0m, in \u001b[0;36mCustomSoundStreamTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m     loss, (recon_loss, \u001b[38;5;241m*\u001b[39m_) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoundstream(wave, return_loss_breakdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every)\n\u001b[1;32m    249\u001b[0m     accum_log(logs, \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m--> 250\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every,\n\u001b[1;32m    251\u001b[0m         recon_loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every\n\u001b[1;32m    252\u001b[0m     ))\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exists(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoundstream\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "from audiolm_pytorch.data import SoundDataset\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "DATA_MAX_LENGTH = 320 * 32\n",
    "\n",
    "dataset = MusicDataset(\n",
    "    '/media/philip/ferous/FMA/fma_small/', \n",
    "    max_length=DATA_MAX_LENGTH,\n",
    "    target_sample_hz = soundstream.target_sample_hz,\n",
    "    seq_len_multiple_of = soundstream.seq_len_multiple_of\n",
    ")\n",
    "\n",
    "trainer = CustomSoundStreamTrainer(\n",
    "    soundstream,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length = DATA_MAX_LENGTH,\n",
    "    num_train_steps = 1000,\n",
    "    dataset=dataset\n",
    ").cuda()\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
