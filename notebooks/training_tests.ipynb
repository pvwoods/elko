{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ff4fbb05-79f9-4bad-ac0c-cf6dbb507817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolm_pytorch.data import exists, cast_tuple, collate_one_or_multiple_tensors, curtail_to_multiple, curtail_to_shortest_collate\n",
    "from functools import partial, wraps\n",
    "\n",
    "from beartype.typing import Tuple\n",
    "from beartype.door import is_bearable\n",
    "\n",
    "import torchaudio\n",
    "from torchaudio.functional import resample\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from audiolm_pytorch.utils import curtail_to_multiple\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        exts = ['mp3', 'wav'],\n",
    "        max_length = None,\n",
    "        target_sample_hz = None,\n",
    "        seq_len_multiple_of = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        path = Path(folder)\n",
    "        assert path.exists(), 'folder does not exist'\n",
    "\n",
    "        files = [os.path.join(root, name)\n",
    "             for root, dirs, files in os.walk(path)\n",
    "             for name in files\n",
    "             if name.endswith(tuple(exts))]\n",
    "        assert len(files) > 0, 'no sound files found'\n",
    "\n",
    "        self.files = files\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.target_sample_hz = cast_tuple(target_sample_hz)\n",
    "        self.seq_len_multiple_of = seq_len_multiple_of\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        \n",
    "        try:\n",
    "            data, sample_hz = torchaudio.load(file) # FMA dataset is a mess, so just jumping over bad loads for now\n",
    "        except e:\n",
    "            return\n",
    "        \n",
    "        if exists(self.target_sample_hz) and self.target_sample_hz[0] != sample_hz:\n",
    "            data = resample(data, orig_freq=sample_hz, new_freq=self.target_sample_hz[0])\n",
    "        \n",
    "        if data.shape[0] != 1:\n",
    "            data = data.mean(dim=0, keepdims=True)\n",
    "        \n",
    "        if data.size(1) > self.max_length:\n",
    "            max_start = data.size(1) - self.max_length\n",
    "            start = torch.randint(0, max_start, (1, ))\n",
    "            data = data[:, start:start + self.max_length]\n",
    "\n",
    "        else:\n",
    "            data = torch.nn.functional.pad(data, (0, self.max_length - data.size(1)), 'constant')\n",
    "        \n",
    "        data = rearrange(data, '1 ... -> ...')\n",
    "\n",
    "        num_outputs = len(self.target_sample_hz)\n",
    "        data = cast_tuple(data, num_outputs)\n",
    "\n",
    "        if exists(self.max_length):\n",
    "            data = tuple(d[:self.max_length] for d in data)\n",
    "\n",
    "        if exists(self.seq_len_multiple_of):\n",
    "            data = tuple(curtail_to_multiple(d, self.seq_len_multiple_of) for d in data)\n",
    "\n",
    "        data = tuple(d.float() for d in data)\n",
    "\n",
    "        if num_outputs == 1:\n",
    "            return data[0]\n",
    "\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64eba6c3-6423-41be-806a-e3626ec0445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import copy\n",
    "from random import choice\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "\n",
    "from beartype.typing import Union, List, Optional, Tuple\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from beartype import beartype\n",
    "from beartype.door import is_bearable\n",
    "from beartype.vale import Is\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from audiolm_pytorch.optimizer import get_optimizer\n",
    "\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from audiolm_pytorch.soundstream import SoundStream\n",
    "\n",
    "from audiolm_pytorch.audiolm_pytorch import (\n",
    "    SemanticTransformer,\n",
    "    SemanticTransformerWrapper,\n",
    "    CoarseTransformer,\n",
    "    CoarseTransformerWrapper,\n",
    "    FineTransformer,\n",
    "    FineTransformerWrapper,\n",
    "    FairseqVQWav2Vec,\n",
    "    HubertWithKmeans\n",
    ")\n",
    "\n",
    "from audiolm_pytorch.data import SoundDataset, get_dataloader\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from audiolm_pytorch.trainer import (\n",
    "    noop,\n",
    "    cycle,\n",
    "    yes_or_no,\n",
    "    accum_log,\n",
    "    has_duplicates,\n",
    "    determine_types,\n",
    "    DEFAULT_SAMPLE_RATE\n",
    ")\n",
    "\n",
    "class CustomSoundStreamTrainer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        soundstream: SoundStream,\n",
    "        *,\n",
    "        num_train_steps,\n",
    "        batch_size,\n",
    "        data_max_length = None,\n",
    "        folder='',\n",
    "        lr = 3e-4,\n",
    "        grad_accum_every = 4,\n",
    "        wd = 0.,\n",
    "        max_grad_norm = 0.5,\n",
    "        discr_max_grad_norm = None,\n",
    "        save_results_every = 100,\n",
    "        save_model_every = 1000,\n",
    "        results_folder = './results',\n",
    "        valid_frac = 0.05,\n",
    "        random_split_seed = 42,\n",
    "        ema_beta = 0.995,\n",
    "        ema_update_after_step = 500,\n",
    "        ema_update_every = 10,\n",
    "        apply_grad_penalty_every = 4,\n",
    "        accelerate_kwargs: dict = dict(),\n",
    "        dataset:Dataset = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.accelerator = Accelerator(**accelerate_kwargs)\n",
    "\n",
    "        self.soundstream = soundstream\n",
    "        self.ema_soundstream = EMA(soundstream, beta = ema_beta, update_after_step = ema_update_after_step, update_every = ema_update_every)\n",
    "\n",
    "        self.register_buffer('steps', torch.Tensor([0]))\n",
    "\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.grad_accum_every = grad_accum_every\n",
    "\n",
    "        # optimizers\n",
    "\n",
    "        self.optim = get_optimizer(soundstream.non_discr_parameters(), lr = lr, wd = wd)\n",
    "\n",
    "        for discr_optimizer_key, discr in self.multiscale_discriminator_iter():\n",
    "            one_multiscale_discr_optimizer = get_optimizer(discr.parameters(), lr = lr, wd = wd)\n",
    "            setattr(self, discr_optimizer_key, one_multiscale_discr_optimizer)\n",
    "\n",
    "        self.discr_optim = get_optimizer(soundstream.stft_discriminator.parameters(), lr = lr, wd = wd)\n",
    "\n",
    "        # max grad norm\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.discr_max_grad_norm = discr_max_grad_norm\n",
    "\n",
    "        # create dataset\n",
    "        \n",
    "        if dataset is not None:\n",
    "            self.ds = dataset\n",
    "        else:\n",
    "            self.ds = SoundDataset(\n",
    "                folder,\n",
    "                max_length = data_max_length,\n",
    "                target_sample_hz = soundstream.target_sample_hz,\n",
    "                seq_len_multiple_of = soundstream.seq_len_multiple_of\n",
    "            )\n",
    "\n",
    "        # split for validation\n",
    "\n",
    "        if valid_frac > 0:\n",
    "            train_size = int((1 - valid_frac) * len(self.ds))\n",
    "            valid_size = len(self.ds) - train_size\n",
    "            self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n",
    "            self.print(f'training with dataset of {len(self.ds)} samples and validating with randomly splitted {len(self.valid_ds)} samples')\n",
    "        else:\n",
    "            self.valid_ds = self.ds\n",
    "            self.print(f'training with shared training and valid dataset of {len(self.ds)} samples')\n",
    "\n",
    "        # dataloader\n",
    "\n",
    "        self.dl = get_dataloader(self.ds, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        self.valid_dl = get_dataloader(self.valid_ds, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "        # prepare with accelerator\n",
    "\n",
    "        (\n",
    "            self.soundstream,\n",
    "            self.optim,\n",
    "            self.discr_optim,\n",
    "            self.dl,\n",
    "            self.valid_dl\n",
    "        ) = self.accelerator.prepare(\n",
    "            self.soundstream,\n",
    "            self.optim,\n",
    "            self.discr_optim,\n",
    "            self.dl,\n",
    "            self.valid_dl\n",
    "        )\n",
    "\n",
    "        # prepare the multiscale discriminators with accelerator\n",
    "\n",
    "        for name, _ in self.multiscale_discriminator_iter():\n",
    "            optimizer = getattr(self, name)\n",
    "            optimizer = self.accelerator.prepare(optimizer)\n",
    "            setattr(self, name, optimizer)\n",
    "\n",
    "        # dataloader iterators\n",
    "\n",
    "        self.dl_iter = cycle(self.dl)\n",
    "        self.valid_dl_iter = cycle(self.valid_dl)\n",
    "\n",
    "        self.save_model_every = save_model_every\n",
    "        self.save_results_every = save_results_every\n",
    "\n",
    "        self.apply_grad_penalty_every = apply_grad_penalty_every\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "\n",
    "        if len([*self.results_folder.glob('**/*')]) > 0 and yes_or_no('do you want to clear previous experiment checkpoints and results?'):\n",
    "            rmtree(str(self.results_folder))\n",
    "\n",
    "        self.results_folder.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    def save(self, path):\n",
    "        pkg = dict(\n",
    "            model = self.accelerator.get_state_dict(self.soundstream),\n",
    "            ema_model = self.ema_soundstream.state_dict(),\n",
    "            optim = self.optim.state_dict(),\n",
    "            discr_optim = self.discr_optim.state_dict()\n",
    "        )\n",
    "\n",
    "        for key, _ in self.multiscale_discriminator_iter():\n",
    "            discr_optim = getattr(self, key)\n",
    "            pkg[key] = discr_optim.state_dict()\n",
    "\n",
    "        torch.save(pkg, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        path = Path(path)\n",
    "        assert path.exists()\n",
    "        pkg = torch.load(str(path))\n",
    "\n",
    "        soundstream = self.accelerator.unwrap_model(self.soundstream)\n",
    "        soundstream.load_state_dict(pkg['model'])\n",
    "\n",
    "        self.ema_soundstream.load_state_dict(pkg['ema_model'])\n",
    "        self.optim.load_state_dict(pkg['optim'])\n",
    "        self.discr_optim.load_state_dict(pkg['discr_optim'])\n",
    "\n",
    "        for key, _ in self.multiscale_discriminator_iter():\n",
    "            discr_optim = getattr(self, key)\n",
    "            discr_optim.load_state_dict(pkg[key])\n",
    "\n",
    "    def multiscale_discriminator_iter(self):\n",
    "        for ind, discr in enumerate(self.soundstream.discriminators):\n",
    "            yield f'multiscale_discr_optimizer_{ind}', discr\n",
    "\n",
    "    def print(self, msg):\n",
    "        self.accelerator.print(msg)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    @property\n",
    "    def is_distributed(self):\n",
    "        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)\n",
    "\n",
    "    @property\n",
    "    def is_main(self):\n",
    "        return self.accelerator.is_main_process\n",
    "\n",
    "    @property\n",
    "    def is_local_main(self):\n",
    "        return self.accelerator.is_local_main_process\n",
    "\n",
    "    def train_step(self):\n",
    "        device = self.device\n",
    "\n",
    "        steps = int(self.steps.item())\n",
    "        apply_grad_penalty = not (steps % self.apply_grad_penalty_every)\n",
    "\n",
    "        self.soundstream.train()\n",
    "\n",
    "        # logs\n",
    "\n",
    "        logs = {}\n",
    "\n",
    "        # update vae (generator)\n",
    "\n",
    "        for _ in range(self.grad_accum_every):\n",
    "            wave, = next(self.dl_iter)\n",
    "            wave = wave.to(device)\n",
    "\n",
    "            loss, (recon_loss, *_) = self.soundstream(wave, return_loss_breakdown = True)\n",
    "\n",
    "            self.accelerator.backward(loss / self.grad_accum_every)\n",
    "\n",
    "            accum_log(logs, dict(\n",
    "                loss = loss.item() / self.grad_accum_every,\n",
    "                recon_loss = recon_loss / self.grad_accum_every\n",
    "            ))\n",
    "\n",
    "        if exists(self.max_grad_norm):\n",
    "            self.accelerator.clip_grad_norm_(self.soundstream.parameters(), self.max_grad_norm)\n",
    "\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        # update discriminator\n",
    "\n",
    "        for _ in range(self.grad_accum_every):\n",
    "            wave, = next(self.dl_iter)\n",
    "            wave = wave.to(device)\n",
    "\n",
    "            discr_losses = self.soundstream(\n",
    "                wave,\n",
    "                apply_grad_penalty = apply_grad_penalty,\n",
    "                return_discr_loss = True,\n",
    "                return_discr_losses_separately = True\n",
    "            )\n",
    "\n",
    "            for name, discr_loss in discr_losses:\n",
    "                self.accelerator.backward(discr_loss / self.grad_accum_every, retain_graph = True)\n",
    "                accum_log(logs, {name: discr_loss.item() / self.grad_accum_every})\n",
    "\n",
    "        if exists(self.discr_max_grad_norm):\n",
    "            self.accelerator.clip_grad_norm_(self.soundstream.stft_discriminator.parameters(), self.discr_max_grad_norm)\n",
    "\n",
    "        # gradient step for all discriminators\n",
    "\n",
    "        self.discr_optim.step()\n",
    "        self.discr_optim.zero_grad()\n",
    "\n",
    "        for ind in range(len(self.soundstream.discriminators)):\n",
    "            discr_optimizer = getattr(self, f'multiscale_discr_optimizer_{ind}')\n",
    "            discr_optimizer.step()\n",
    "            discr_optimizer.zero_grad()\n",
    "\n",
    "        # build pretty printed losses\n",
    "\n",
    "        losses_str = f\"{steps}: soundstream total loss: {logs['loss']:.3f}, soundstream recon loss: {logs['recon_loss']:.3f}\"\n",
    "\n",
    "        for key, loss in logs.items():\n",
    "            if not key.startswith('scale:'):\n",
    "                continue\n",
    "            _, scale_factor = key.split(':')\n",
    "\n",
    "            losses_str += f\" | discr (scale {scale_factor}) loss: {loss:.3f}\"\n",
    "\n",
    "        # log\n",
    "\n",
    "        self.print(losses_str)\n",
    "\n",
    "        # update exponential moving averaged generator\n",
    "\n",
    "        if self.is_main:\n",
    "            self.ema_soundstream.update()\n",
    "\n",
    "        # sample results every so often\n",
    "\n",
    "        if self.is_main and not (steps % self.save_results_every):\n",
    "            for model, filename in ((self.ema_soundstream.ema_model, f'{steps}.ema'), (self.soundstream, str(steps))):\n",
    "                model.eval()\n",
    "\n",
    "                wave, = next(self.valid_dl_iter)\n",
    "                wave = wave.to(device)\n",
    "\n",
    "                recons = model(wave, return_recons_only = True)\n",
    "\n",
    "                milestone = steps // self.save_results_every\n",
    "\n",
    "                for ind, recon in enumerate(recons.unbind(dim = 0)):\n",
    "                    filename = str(self.results_folder / f'sample_{steps}.flac')\n",
    "                    torchaudio.save(filename, recon.cpu().detach(), DEFAULT_SAMPLE_RATE)\n",
    "\n",
    "            self.print(f'{steps}: saving to {str(self.results_folder)}')\n",
    "\n",
    "        # save model every so often\n",
    "\n",
    "        if self.is_main and not (steps % self.save_model_every):\n",
    "            state_dict = self.soundstream.state_dict()\n",
    "            model_path = str(self.results_folder / f'soundstream.{steps}.pt')\n",
    "            torch.save(state_dict, model_path)\n",
    "\n",
    "            ema_state_dict = self.ema_soundstream.state_dict()\n",
    "            model_path = str(self.results_folder / f'soundstream.{steps}.ema.pt')\n",
    "            torch.save(ema_state_dict, model_path)\n",
    "\n",
    "            self.print(f'{steps}: saving model to {str(self.results_folder)}')\n",
    "\n",
    "        self.steps += 1\n",
    "        return logs\n",
    "\n",
    "    def train(self, log_fn = noop):\n",
    "\n",
    "        while self.steps < self.num_train_steps:\n",
    "            logs = self.train_step()\n",
    "            log_fn(logs)\n",
    "\n",
    "        self.print('training complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da749fab-fbd4-4e54-87d9-f9f0acb1cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 7597 samples and validating with randomly splitted 400 samples\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "do you want to clear previous experiment checkpoints and results? (y/n)  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: soundstream total loss: 24.334, soundstream recon loss: 0.042 | discr (scale 1) loss: 2.000 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 2.000\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: soundstream total loss: 26.065, soundstream recon loss: 0.045 | discr (scale 1) loss: 1.995 | discr (scale 0.5) loss: 1.995 | discr (scale 0.25) loss: 1.996\n",
      "2: soundstream total loss: 31.013, soundstream recon loss: 0.052 | discr (scale 1) loss: 1.984 | discr (scale 0.5) loss: 1.984 | discr (scale 0.25) loss: 1.989\n",
      "3: soundstream total loss: 30.583, soundstream recon loss: 0.049 | discr (scale 1) loss: 1.971 | discr (scale 0.5) loss: 1.985 | discr (scale 0.25) loss: 1.983\n",
      "4: soundstream total loss: 24.303, soundstream recon loss: 0.033 | discr (scale 1) loss: 1.959 | discr (scale 0.5) loss: 2.000 | discr (scale 0.25) loss: 1.981\n",
      "5: soundstream total loss: 21.495, soundstream recon loss: 0.034 | discr (scale 1) loss: 1.950 | discr (scale 0.5) loss: 2.011 | discr (scale 0.25) loss: 1.976\n",
      "6: soundstream total loss: 31.592, soundstream recon loss: 0.053 | discr (scale 1) loss: 1.844 | discr (scale 0.5) loss: 1.952 | discr (scale 0.25) loss: 1.920\n",
      "7: soundstream total loss: 28.808, soundstream recon loss: 0.048 | discr (scale 1) loss: 1.799 | discr (scale 0.5) loss: 1.918 | discr (scale 0.25) loss: 1.889\n",
      "8: soundstream total loss: 25.408, soundstream recon loss: 0.037 | discr (scale 1) loss: 1.616 | discr (scale 0.5) loss: 1.799 | discr (scale 0.25) loss: 1.807\n",
      "9: soundstream total loss: 28.932, soundstream recon loss: 0.049 | discr (scale 1) loss: 1.509 | discr (scale 0.5) loss: 1.726 | discr (scale 0.25) loss: 1.745\n",
      "10: soundstream total loss: 20.920, soundstream recon loss: 0.029 | discr (scale 1) loss: 1.310 | discr (scale 0.5) loss: 1.607 | discr (scale 0.25) loss: 1.601\n",
      "11: soundstream total loss: 28.651, soundstream recon loss: 0.068 | discr (scale 1) loss: 1.273 | discr (scale 0.5) loss: 1.599 | discr (scale 0.25) loss: 1.572\n",
      "12: soundstream total loss: 21.076, soundstream recon loss: 0.035 | discr (scale 1) loss: 1.179 | discr (scale 0.5) loss: 1.487 | discr (scale 0.25) loss: 1.489\n",
      "13: soundstream total loss: 20.819, soundstream recon loss: 0.045 | discr (scale 1) loss: 0.893 | discr (scale 0.5) loss: 1.072 | discr (scale 0.25) loss: 1.181\n",
      "14: soundstream total loss: 29.515, soundstream recon loss: 0.060 | discr (scale 1) loss: 0.954 | discr (scale 0.5) loss: 0.798 | discr (scale 0.25) loss: 0.911\n",
      "15: soundstream total loss: 24.611, soundstream recon loss: 0.033 | discr (scale 1) loss: 0.547 | discr (scale 0.5) loss: 0.984 | discr (scale 0.25) loss: 0.682\n",
      "16: soundstream total loss: 23.894, soundstream recon loss: 0.049 | discr (scale 1) loss: 0.454 | discr (scale 0.5) loss: 1.100 | discr (scale 0.25) loss: 0.992\n",
      "17: soundstream total loss: 21.300, soundstream recon loss: 0.031 | discr (scale 1) loss: 1.053 | discr (scale 0.5) loss: 0.746 | discr (scale 0.25) loss: 0.594\n",
      "18: soundstream total loss: 23.068, soundstream recon loss: 0.043 | discr (scale 1) loss: 0.629 | discr (scale 0.5) loss: 0.752 | discr (scale 0.25) loss: 0.762\n",
      "19: soundstream total loss: 23.686, soundstream recon loss: 0.046 | discr (scale 1) loss: 0.658 | discr (scale 0.5) loss: 2.366 | discr (scale 0.25) loss: 1.177\n",
      "20: soundstream total loss: 24.940, soundstream recon loss: 0.042 | discr (scale 1) loss: 1.438 | discr (scale 0.5) loss: 2.290 | discr (scale 0.25) loss: 0.860\n",
      "21: soundstream total loss: 26.241, soundstream recon loss: 0.049 | discr (scale 1) loss: 1.268 | discr (scale 0.5) loss: 1.488 | discr (scale 0.25) loss: 1.440\n",
      "22: soundstream total loss: 27.529, soundstream recon loss: 0.054 | discr (scale 1) loss: 1.208 | discr (scale 0.5) loss: 1.469 | discr (scale 0.25) loss: 1.415\n",
      "23: soundstream total loss: 28.061, soundstream recon loss: 0.058 | discr (scale 1) loss: 2.044 | discr (scale 0.5) loss: 1.503 | discr (scale 0.25) loss: 1.544\n",
      "24: soundstream total loss: 23.447, soundstream recon loss: 0.045 | discr (scale 1) loss: 1.255 | discr (scale 0.5) loss: 2.426 | discr (scale 0.25) loss: 1.455\n",
      "25: soundstream total loss: 24.769, soundstream recon loss: 0.047 | discr (scale 1) loss: 1.247 | discr (scale 0.5) loss: 2.303 | discr (scale 0.25) loss: 0.752\n",
      "26: soundstream total loss: 28.258, soundstream recon loss: 0.062 | discr (scale 1) loss: 1.581 | discr (scale 0.5) loss: 0.888 | discr (scale 0.25) loss: 0.974\n",
      "27: soundstream total loss: 29.457, soundstream recon loss: 0.059 | discr (scale 1) loss: 0.623 | discr (scale 0.5) loss: 1.165 | discr (scale 0.25) loss: 0.462\n",
      "28: soundstream total loss: 32.177, soundstream recon loss: 0.079 | discr (scale 1) loss: 0.353 | discr (scale 0.5) loss: 0.988 | discr (scale 0.25) loss: 0.258\n",
      "29: soundstream total loss: 31.873, soundstream recon loss: 0.070 | discr (scale 1) loss: 0.153 | discr (scale 0.5) loss: 0.452 | discr (scale 0.25) loss: 0.335\n",
      "30: soundstream total loss: 28.153, soundstream recon loss: 0.072 | discr (scale 1) loss: 0.166 | discr (scale 0.5) loss: 1.190 | discr (scale 0.25) loss: 0.496\n",
      "31: soundstream total loss: 26.447, soundstream recon loss: 0.054 | discr (scale 1) loss: 0.838 | discr (scale 0.5) loss: 1.452 | discr (scale 0.25) loss: 0.187\n",
      "32: soundstream total loss: 33.200, soundstream recon loss: 0.073 | discr (scale 1) loss: 0.513 | discr (scale 0.5) loss: 0.725 | discr (scale 0.25) loss: 1.674\n",
      "33: soundstream total loss: 31.572, soundstream recon loss: 0.055 | discr (scale 1) loss: 1.388 | discr (scale 0.5) loss: 3.112 | discr (scale 0.25) loss: 1.496\n",
      "34: soundstream total loss: 32.764, soundstream recon loss: 0.064 | discr (scale 1) loss: 2.014 | discr (scale 0.5) loss: 3.992 | discr (scale 0.25) loss: 2.003\n",
      "35: soundstream total loss: 27.109, soundstream recon loss: 0.051 | discr (scale 1) loss: 2.911 | discr (scale 0.5) loss: 5.153 | discr (scale 0.25) loss: 2.812\n",
      "36: soundstream total loss: 27.230, soundstream recon loss: 0.054 | discr (scale 1) loss: 4.236 | discr (scale 0.5) loss: 5.750 | discr (scale 0.25) loss: 3.428\n"
     ]
    }
   ],
   "source": [
    "from audiolm_pytorch import SoundStream, SoundStreamTrainer\n",
    "from audiolm_pytorch.data import SoundDataset\n",
    "\n",
    "soundstream = SoundStream(\n",
    "    codebook_size = 1024,\n",
    "    rq_num_quantizers = 8,\n",
    ")\n",
    "DATA_MAX_LENGTH = 320 * 32\n",
    "\n",
    "dataset = MusicDataset(\n",
    "    '/media/philip/ferous/FMA/fma_small/', \n",
    "    max_length=DATA_MAX_LENGTH,\n",
    "    target_sample_hz = soundstream.target_sample_hz,\n",
    "    seq_len_multiple_of = soundstream.seq_len_multiple_of\n",
    ")\n",
    "\n",
    "trainer = CustomSoundStreamTrainer(\n",
    "    soundstream,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 8,         # effective batch size of 32\n",
    "    data_max_length = DATA_MAX_LENGTH,\n",
    "    num_train_steps = 1000,\n",
    "    dataset=dataset\n",
    ").cuda()\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
